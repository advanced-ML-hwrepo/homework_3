{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "In this homeowork, we will\n",
    "- Build a neural network based part of speech (POS) tagger [35 points].\n",
    "- Run the model on a GPU at Google Colab [10].\n",
    "- Extend the model to use pretrained embeddings [15].\n",
    "- Extend the model in some manner of your own choosing [30].\n",
    "\n",
    "There are a total of 8 tasks.\n",
    "\n",
    "Acknowledgements: Part of this homework were adapted from work done by\n",
    "[Ben Trevett](https://github.com/bentrevett) and\n",
    "[Kevin Gimpel](https://home.ttic.edu/~kgimpel/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing libraries\n",
    "\n",
    "This homework requires you to install the following libraries, which are the current latest versions.  They are not, however, the default versions installed by, say, Anaconda.  The code has been tested on these versions; but it is possible it works on other versions as well.\n",
    "\n",
    "- PyTorch 1.11.0\n",
    "- torchtext 0.12.0\n",
    "- TorchData 0.3.0\n",
    "\n",
    "You may need to read some of the documentation for these libraries to solve this homework.\n",
    "\n",
    "Since these libraries are evolving rapidly, you may get some warnings.  Please ignore them unless the code later breaks down.\n",
    "\n",
    "When working with colab, in a new runtime, you'll have to install the above libraries. (Once installed you usually do not need reinstallation if you, e.g., restart the runtime.)  One way to install them is via the following commands (they automatically update PyTorch).\n",
    "\n",
    "`!pip install torchtext==0.12.0`\n",
    "\n",
    "`!pip install torchdata==0.3.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.11.0', '0.12.0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the library versions\n",
    "torch.__version__, torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 30255 # Specify a seed for reproducability\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(53113)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPUs\n",
    "\n",
    "We want our code to run on a CPU or on a [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit) when available, as on Google Colab.  The code will likely run at least five times as fast on a GPU. PyTorch connects with a GPU via the system interface [CUDA](https://en.wikipedia.org/wiki/CUDA).\n",
    "\n",
    "The following tells us whether CUDA/GPU is available or not.  The changes you have to make so that your code can use a GPU when available are surprisingly small. All such changes have already been made in the provided code below. They are:\n",
    "- the data loader needs to place the input to the model appropriately using `to(device).`\n",
    "- similarly, the model object itself needs to be placed appropriately using `to(device).`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache for GloVe\n",
    "\n",
    "If you have already downloaded GloVe in a directory, you can reuse it.  Please change the following if so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cpu':\n",
    "    VECTORS_CACHE_DIR = '/Users/amitabh/.vector_cache'\n",
    "    # Please change above to your cache\n",
    "else:\n",
    "    VECTORS_CACHE_DIR = './.vector_cache'\n",
    "    # This is the default cache on Colab. Caching may not work\n",
    "    # as expected on Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDPOS Dataset\n",
    "\n",
    "We'll work with the [`UDPOS`](https://pytorch.org/text/stable/datasets.html#udpos) dataset included with torchtext.  It contains about 12,500 sentences and the POS tags for the each word.  There are, in fact, two sets of tags, based on two different POS tagging standards: [Universal Dependency](https://universaldependencies.org/u/pos/) (UD) and [Penn Treebank](https://www.sketchengine.eu/penn-treebank-tagset/) (PTB). It has additionally about 2000 sentences in the validation and test sets.  The words and corresonding tags are organized into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence for Example 0 ---\n",
      "['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']\n",
      "The UD tags for Example 0 ---\n",
      "['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n",
      "The PTB tags for Example 0 ---\n",
      "['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']\n",
      "\n",
      "The sentence for Example 1 ---\n",
      "['[', 'This', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']']\n",
      "The UD tags for Example 1 ---\n",
      "['PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'AUX', 'AUX', 'VERB', 'PRON', 'NOUN', 'ADP', 'NOUN', 'PART', 'VERB', 'PUNCT', 'PUNCT']\n",
      "The PTB tags for Example 1 ---\n",
      "['-LRB-', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'MD', 'VB', 'VBG', 'PRP', 'NN', 'IN', 'NNS', 'TO', 'VB', '.', '-RRB-']\n",
      "\n",
      "The sentence for Example 2 ---\n",
      "['DPA', ':', 'Iraqi', 'authorities', 'announced', 'that', 'they', 'had', 'busted', 'up', '3', 'terrorist', 'cells', 'operating', 'in', 'Baghdad', '.']\n",
      "The UD tags for Example 2 ---\n",
      "['PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'SCONJ', 'PRON', 'AUX', 'VERB', 'ADP', 'NUM', 'ADJ', 'NOUN', 'VERB', 'ADP', 'PROPN', 'PUNCT']\n",
      "The PTB tags for Example 2 ---\n",
      "['NNP', ':', 'JJ', 'NNS', 'VBD', 'IN', 'PRP', 'VBD', 'VBN', 'RP', 'CD', 'JJ', 'NNS', 'VBG', 'IN', 'NNP', '.']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amitabh/miniforge3/lib/python3.9/site-packages/torch/utils/data/datapipes/utils/common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\n",
      "/Users/amitabh/miniforge3/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\"Lambda function is not supported for pickle, please use \"\n"
     ]
    }
   ],
   "source": [
    "train_iter = datasets.UDPOS(split = 'train')\n",
    "\n",
    "for i, example in enumerate(train_iter):\n",
    "    print(f'The sentence for Example {i} ---')\n",
    "    print(example[0])\n",
    "    print(f'The UD tags for Example {i} ---')\n",
    "    print(example[1])\n",
    "    print(f'The PTB tags for Example {i} ---')\n",
    "    print(example[2])\n",
    "    print()\n",
    "    if i == 2: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "In our neural network-based model we can only accept fixed sized inputs.  Each input corresponds to a subsequence, consisting of a  center word and a number $w$ of words before and after, and the label is the POS tag for the center word. We often refer to the subsequence as a window of size $(2w + 1)$.\n",
    "\n",
    "So given a sentence, say, \"This killing of a respected cleric.' with, tags, say 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT' (the last tag is for the period at the end of the sentence), some of our input examples for the model are the following (for $w = 1$):\n",
    "- \"This killing of\", 'NOUN'\n",
    "- \"killing of a\", 'ADP'\n",
    "- \"respected cleric.\", 'NOUN'.\n",
    "\n",
    "But the above scheme would imply that, when $w = 1$, the tags of the first and last words for any sentence would never be part of an example.  To avoid that, we add dummy '\\<s>' and '\\</s>' words, and corresponding tags, to the beginning and end of each sentence.  We need to add as many dummy words as $w$.  So if $w = 2$, we first convert the above sentence and its tags to\n",
    "- \"\\<s> \\<s> This killing of a respected cleric. \\</s> \\</s>' \n",
    "- 'STAG', 'STAG','DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'ETAG', 'ETAG'. \n",
    "\n",
    "**Task 1** [5 points]: Most of the code for preprocessing is given below.  But some parts are missing.  Complete the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 1\n",
    "WINDOW_SIZE = (2 * W + 1)\n",
    "\n",
    "SENT_START_WORD = '<s>'\n",
    "SENT_END_WORD = '</s>'\n",
    "SENT_START_TAG = '<STAG>'\n",
    "SENT_END_TAG = '<ETAG>'\n",
    "\n",
    "\n",
    "def add_sent_start_end(data_iter, w):\n",
    "    for (words, ud_tags, ptb_tags) in data_iter:\n",
    "        new_words = [SENT_START_WORD] * w + words + [SENT_END_WORD] * w\n",
    "        new_ud_tags = [SENT_START_TAG] * w + ud_tags + [SENT_END_TAG] * w\n",
    "        ## MISSING PART: ADD YOUR CODE BELOW\n",
    "       \n",
    "        ## ADD YOUR CODE ABOVE\n",
    "        yield(new_words, new_ud_tags, new_ptb_tags)\n",
    "        \n",
    "def create_windows(data_iter, w):\n",
    "    window_size = 2*w + 1\n",
    "    for (words, ud_tags, ptb_tags) in data_iter:\n",
    "        words_zip = zip(*[words[i:] for i in range(window_size)])\n",
    "        ud_zip = zip(*[ud_tags[i:] for i in range(window_size)])\n",
    "        ## MISSING PART: ADD YOUR CODE BELOW\n",
    "       \n",
    "        ## ADD YOUR CODE ABOVE\n",
    "        for word_sseq, ud_sseq, ptb_sseq in zip(\n",
    "                words_zip, ud_zip, ptb_zip):\n",
    "            yield(word_sseq, ud_sseq, ptb_sseq)\n",
    "            \n",
    "def preprocess_data_seq(data_iter, w):\n",
    "    ## MISSING PART: ADD YOUR CODE BELOW\n",
    "    pass\n",
    "\n",
    "def test_preprocess_data_seq():\n",
    "    \n",
    "    # WARNING: The following test assumes a particular default\n",
    "    # sequence of examples in the PyTorch UDPOS dataset. If you\n",
    "    # suspect the sequence is different for your dataset, please\n",
    "    # adapt the test.\n",
    "\n",
    "    train_iter_0 = datasets.UDPOS(split = 'train')    \n",
    "    train_iter_demo = preprocess_data_seq(train_iter_0, 1)\n",
    "    ex0 = (('<s>', 'Al', '-'), \n",
    "           ('<STAG>', 'PROPN', 'PUNCT'), \n",
    "           ('<STAG>', 'NNP', 'HYPH'))\n",
    "    ex1 = (('Al', '-', 'Zaman'), \n",
    "           ('PROPN', 'PUNCT', 'PROPN'), \n",
    "           ('NNP', 'HYPH', 'NNP'))\n",
    "    ex2 = (('-', 'Zaman', ':'), \n",
    "           ('PUNCT', 'PROPN', 'PUNCT'), \n",
    "           ('HYPH', 'NNP', ':'))\n",
    "    assert ex0 == next(train_iter_demo)\n",
    "    assert ex1 == next(train_iter_demo)\n",
    "    assert ex2 == next(train_iter_demo)\n",
    "    print('Test passed.')\n",
    "    \n",
    "test_preprocess_data_seq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabularies for words and tags\n",
    "\n",
    "Since the number of tags is reasonably large, we use the `Vocab` class to create objects `vocab_words`, `vocab_ud`, and `vocab_ptb` for both words and the two sets of tags from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is important to recreate the training set afresh \n",
    "# each time they are used, since they are Python iterators, and\n",
    "# once use, cannot be reused.  One could, however, cast them into\n",
    "# a list, which would store them permanently.\n",
    "\n",
    "train_iter_0 = datasets.UDPOS(split = 'train')    \n",
    "train_iter_vocab = preprocess_data_seq(train_iter_0, 1)\n",
    "\n",
    "counter_words = Counter()\n",
    "counter_ud = Counter()\n",
    "counter_ptb = Counter()\n",
    "for (text, pos_ud, pos_ptb) in train_iter_vocab:\n",
    "    counter_words.update(text)\n",
    "    counter_ud.update(pos_ud)\n",
    "    counter_ptb.update(pos_ptb)\n",
    "vocab_words = vocab(counter_words,  specials = ['<unk>'], \n",
    "                    special_first = True)    \n",
    "vocab_words.set_default_index(0)\n",
    "vocab_ud = vocab(counter_ud)\n",
    "vocab_ptb = vocab(counter_ptb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we set a special word for vocab_words, but none for vocab_ud or vocab_ptb.  Why is that? (You don't have to submit an answer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate function for customized data loader\n",
    "\n",
    "**Task 2** [10]: Write a collate function that takes a batch of examples, and for a given window width and tag type, returns the following\n",
    "- a tensor corresponding to the index of the tag for the center word in each example of the batch. Recall, each example is a window with a width of $(2w + 1)$ words.\n",
    "- a tensor corresponding to the index of each of the words in the example, according the to vocabulary for words.\n",
    "\n",
    "Note: The collate function is called by the data loader with the batch of examples from the data set.  So any additional parameters to the function should have default values.\n",
    "\n",
    "From this task onwards we'll only work with the UD tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "TAG = 'ud'\n",
    "\n",
    "def collate_fn(batch, w = W, tag = TAG):\n",
    "    \n",
    "    ## WRITE YOUR CODE BELOW\n",
    "    \n",
    "    ## WRITE YOUR CODE ABOVE\n",
    "    # The tensors you return should be placed in the correct device\n",
    "    # as shown below.\n",
    "    return labels.to(device), word_idxs.to(device)\n",
    "\n",
    "\n",
    "def test_collate():\n",
    "    \n",
    "    pos = [5, 6, 1, 4]  \n",
    "    examples = []\n",
    "    for perm in ['03022', '33210', '33211', '11101']:\n",
    "        words = []\n",
    "        utags = []\n",
    "        ptags = []\n",
    "        for ind in perm:\n",
    "            ind = int(ind)\n",
    "            words.append(vocab_words.lookup_token(pos[ind]))\n",
    "            utags.append(vocab_ud.lookup_token(pos[ind]+1))\n",
    "            ptags.append(vocab_ptb.lookup_token(pos[ind]))\n",
    "        examples.append((words, utags, ptags))\n",
    "    lt =  torch.tensor([5, 1, 1, 6]).to(device)\n",
    "    lt =  torch.tensor([6, 2, 2, 7]).to(device)\n",
    "    wt = torch.tensor([\n",
    "        [5, 4, 5, 1, 1],\n",
    "        [4, 4, 1, 6, 5],\n",
    "        [4, 4, 1, 6, 6],\n",
    "        [6, 6, 6, 5, 6]]).to(device)\n",
    "    rlt, rwt = collate_fn(examples, w = 2)\n",
    "    assert torch.equal(lt, rlt)\n",
    "    assert torch.equal(wt, rwt)\n",
    "    print('test_collate passed')\n",
    "\n",
    "test_collate()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the class for the neural network model\n",
    "\n",
    "**Task 3** [15]: Write a class that implements the following neural network for input $(w_1, w_2, w_3)$ and label $y$ corresponding to tag of $w_2$. (Also discussed in class on April 15, 2021).\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "x &=& [E_{[w_1]}, E_{[w_2]}, E_{[w_3]}] \\\\\n",
    "h &=& \\text{tanh}(x W^1 + b^1)\\\\\n",
    "\\tilde{y} & = & hW^2 + b^2\\\\\n",
    "\\hat{y} & = & \\text{softmax}(y)\\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "The model returns $\\ln \\hat{y}$, and expects the training loop to use `nn.NLLLoss()` to compute the cross entropy loss, given $y$. Assume the embedding has vectors of size $300$, and $h$ is a vector of size $128$.  Please read the documentation for `nn.Embedding(),` `nn.Linear(),`, and `nn.Tanh()` to learn how to use them.\n",
    "\n",
    "*Hint:* Unlike HW2, here the input (for each example in a batch) consist of $(2w+1)$ numbers corresponding to the indices of the words in the window. These are converted to $(2w+1)$ vectors when they are passed to the embedding layer. They have to be concatenated into a single vector of length $(2w+1) \\times 300$ before they are sent to the first linear layer ($W^1$).  Use `reshape()` or `view()` in the `forward()` function below to achieve this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNPOSTagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 window_size,\n",
    "                 vocab_size, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 nonlinearity, \n",
    "                 # These are used for later tasks\n",
    "                 use_glove = False, \n",
    "                 freeze_glove = False):      \n",
    "        super(NNPOSTagger, self).__init__()\n",
    "        \n",
    "        ## WRITE YOUR CODE BELOW\n",
    "        \n",
    "        pass\n",
    "      \n",
    "        \n",
    "    def forward(self, word_idxs_batch):\n",
    "        \n",
    "        ## WRITE YOUR CODE BELOW.\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating the model\n",
    "\n",
    "PyTorch uses `to(device)` to specify whether the model is to be used with a CPU or with a GPU/CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNPOSTagger(window_size = WINDOW_SIZE, \n",
    "                    vocab_size = len(vocab_words), \n",
    "                     embedding_dim = 300, \n",
    "                     hidden_dim = 128, \n",
    "                     output_dim = len(vocab_ud),\n",
    "                     nonlinearity = nn.Tanh(), \n",
    "                     use_glove = False,\n",
    "                     freeze_glove = False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an epoch\n",
    "\n",
    "**Task 4** [5]: Write a function to train one epoch of the dataset. This is similar to the code in HW2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.NLLLoss()\n",
    "\n",
    "def train_an_epoch(dataloader):\n",
    "    \n",
    "    ### WRITE YOUR CODE BELOW\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the accuracy\n",
    "\n",
    "The function is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():    \n",
    "        total_acc, total_count = 0, 0\n",
    "        for idx, (label, word_idxs) in enumerate(dataloader):\n",
    "            log_probs = model(word_idxs)\n",
    "            total_acc += (log_probs.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # batch size for training\n",
    "  \n",
    "train_0, valid_0, test_0 = train_data_0 = datasets.UDPOS(\n",
    "    split = ('train', 'valid', 'test'))\n",
    "train_data = list(preprocess_data_seq(train_0, W))\n",
    "valid_data = list(preprocess_data_seq(valid_0, W))\n",
    "test_data = list(preprocess_data_seq(test_0, W))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, \n",
    "                              collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, \n",
    "                              collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, \n",
    "                             collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "EPOCHS = 3 # epoch\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "accuracies=[]\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_an_epoch(train_dataloader)\n",
    "    accuracy = get_accuracy(valid_dataloader)\n",
    "    accuracies.append(accuracy)\n",
    "    time_taken = time.time() - epoch_start_time\n",
    "    print(f'Epoch: {epoch}, time taken: {time_taken:.1f}s, validation accuracy: {accuracy:.3f}.')\n",
    "    \n",
    "plt.plot(range(1, EPOCHS+1), accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on Google Colab\n",
    "\n",
    "Google Colab provides GPUs on which we can run Jupyter notebooks. These are for free as long as the job takes less than, say, 4 hours. Our job will take much less.\n",
    "\n",
    "Please create an account on Google if you don't have one.  Please read a tutorial on using Colab (such as [this](https://colab.research.google.com/notebooks/intro.ipynb)).  Upload your notebook on Colab. **Change the runtime type to GPU.** Make any changes required to get your code to run on Colab. (Normally there should  be no change required.) \n",
    "\n",
    "For all following tasks that required running your code for several epochs, we recommend developing your code locally, but finally running it on Colab to get the results.\n",
    "\n",
    "**Task 5** [10]: Run the model for a sufficient number of epochs (not more than 30) such that the model shows overfitting, if at all, and submit a pdf of the plot of accuracy against number of epochs. Determine the optimal number of epochs to train for. Write code to estimate the accuracy of your model (using the test set) corresponding to this optimal number of epocs and report this estimated accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing with pre-trained embeddings\n",
    "\n",
    "In the above model, the embedding matrix is initialized to either zero or random vectors, but trained along with the rest of the parameters. It may help to initialize the embedding matrix from GloVe vector embeddings. You may then choose to freeze the embedding matrix (i.e., not update the vectors during training) or train them to adapt to the POS examples. (For further details you may optionally read Section 10.1--3 from the Goldberg textbook.)\n",
    "\n",
    "The following code snippet creates a embedding matrix that has a vector corresponding to each of our `vocab_words` created from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab\n",
    "\n",
    "glove = vocab.GloVe('6B',cache=VECTORS_CACHE_DIR)\n",
    "glove_vectors = glove.get_vecs_by_tokens(vocab_words.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6** [10]: Rewrite/extend the code for the module to initialize the embedding layer with `glove_vectors` created above.  Please read the documentation for the function [`nn.Embedding.from_pretrained()`](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html#torch.nn.Embedding.from_pretrained) to understand how this is done. Submit corresponding plot and report on the performance when the embedding is frozen.\n",
    "\n",
    "**Task 7** [5]: Submit corresponding plot and report on the performance when the embedding is not frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the model\n",
    "\n",
    "**Task 8** [30]: Try three different ways of your choosing to improve the performance of the model. You may want to vary w, or add additional layers to the network, or increase the size of the hidden vectors $h$, or try with different activation functions. Report on the results you get. Do you think your POS tagger is comparable to a human tagger?\n",
    "\n",
    "This is an open-ended task.  On it you should spend at least half the time as you spend in total on the previous tasks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
